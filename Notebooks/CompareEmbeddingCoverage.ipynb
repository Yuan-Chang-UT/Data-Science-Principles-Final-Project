{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef5e6e9ddb01646b0b776696ecb77ffa81111ff5",
    "colab_type": "text",
    "id": "PRE2syPLPsZV"
   },
   "source": [
    "# EE461P Term Project: Compare Embedding Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "80e3d2aca0697603f1d037cb2d9cc49a3df80a50",
    "colab_type": "text",
    "id": "H03SYnggPsZb"
   },
   "source": [
    "### Embeddings\n",
    "External data sources are not allowed for this competition. We are, though, providing a number of word embeddings along with the dataset that can be used in the models. These are as follows:\n",
    "\n",
    "GoogleNews-vectors-negative300 - https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "glove.840B.300d - https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "paragram_300_sl999 - https://cogcomp.org/page/resource_view/106\n",
    "\n",
    "wiki-news-300d-1M - https://fasttext.cc/docs/en/english-vectors.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "f176435ae03da959c7b16b5b3c75037e2fdcd69a",
    "colab": {},
    "colab_type": "code",
    "id": "mx0aESKVPsZ2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, cross_val_predict, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pprint\n",
    "import operator \n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "056e2d3c5221f1826d32ceb4fe2efe4f9931972b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "7Fuk07ekPsaG",
    "outputId": "db0d1885-c19c-43b8-dfca-97e181a7d2fa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question_text  target\n",
       "0  How did Quebec nationalists see their province...       0\n",
       "1  Do you have an adopted dog, how would you enco...       0\n",
       "2  Why does velocity affect time? Does velocity a...       0\n",
       "3  How did Otto von Guericke used the Magdeburg h...       0\n",
       "4  Can I convert montra helicon D to a mountain b...       0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "train_qid = train['qid']\n",
    "test_qid  = test['qid']\n",
    "y_train = train['target']\n",
    "\n",
    "train.drop(\"qid\", axis = 1, inplace = True)\n",
    "test.drop(\"qid\", axis = 1, inplace = True)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "049139950319155efc61e53022130b2fc903e01c",
    "colab_type": "text",
    "id": "yn9__EnJPsaW"
   },
   "source": [
    "## Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "e8e4c58a959ab591f45537157df76cf0dd43e824",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UIuFCLsDPsaZ",
    "outputId": "85793275-4bd9-4433-c470-c23f0dddc372",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1362492, 1)\n"
     ]
    }
   ],
   "source": [
    "# Concat training and set set for data exploration and feature engineering\n",
    "all_data = pd.DataFrame(pd.concat((train['question_text'], test['question_text'])))\n",
    "\n",
    "# manipulate all_data for any feature engineering, unless you need to compare distributions of train vs test data.\n",
    "print(all_data.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ca96791cbed189f876a1dc3257c35cc0317724c"
   },
   "source": [
    "### Functions for checking coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "a4e5389613fd89f2e7b89493ae61e83794eb77e2"
   },
   "outputs": [],
   "source": [
    "# Counts occurance of words in dataset vocabulary\n",
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "d20aa168a570c4f9ee4b5b20b04975634f1fd41a"
   },
   "outputs": [],
   "source": [
    "# checks the intersection between vocabulary and the embeddings\n",
    "# returns a list of words that can be used to improve preprocessing\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.3%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "acddb1c3be2f97e61a5e445a788d1e07729fe4d2"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# loads embedding from file\n",
    "def load_embed(file):\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    if file == '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin':\n",
    "        embeddings_index = KeyedVectors.load_word2vec_format(file, binary=True)\n",
    "    elif file == '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "994084fac1cf8b37b5b0d1dd2597e8456264762c"
   },
   "source": [
    "### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "7eb0bba6a1d1b9a6b20fdd71c2047ca724b60a51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting GloVe embedding\n"
     ]
    }
   ],
   "source": [
    "glove = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "print(\"Extracting GloVe embedding\")\n",
    "embed_glove = load_embed(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "b3eeddc4160101ec060299b2aaed2d52dddf8813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FastText embedding\n"
     ]
    }
   ],
   "source": [
    "fasttext = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "print(\"Extracting FastText embedding\")\n",
    "embed_fasttext = load_embed(fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "0a46c202f665d6d4fec929341997f373182fb10b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Paragram embedding\n"
     ]
    }
   ],
   "source": [
    "para = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "print(\"Extracting Paragram embedding\")\n",
    "embed_para = load_embed(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd6a6f4fa8fe111159bb703796e5dfcaf3f0e302"
   },
   "source": [
    "### Create default vocab and check base coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "51972f674dd23b4f7c07c01c3a585a307b1eb379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'How': 273144, 'did': 34918, 'Quebec': 102, 'nationalists': 97, 'see': 9397}\n"
     ]
    }
   ],
   "source": [
    "sentences = all_data[\"question_text\"]\n",
    "vocab = build_vocab(sentences)\n",
    "print({k: vocab[k] for k in list(vocab)[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "1085ac6ae46c3e14045892b962315b363b409a17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Coverage: \n",
      "Found embeddings for 32.773% of vocab\n",
      "Found embeddings for  88.149% of all text\n",
      "Fasttext Coverage: \n",
      "Found embeddings for 29.774% of vocab\n",
      "Found embeddings for  87.658% of all text\n",
      "Paragram Coverage: \n",
      "Found embeddings for 19.369% of vocab\n",
      "Found embeddings for  72.205% of all text\n"
     ]
    }
   ],
   "source": [
    "print(\"Glove Coverage: \")\n",
    "oov_glove = check_coverage(vocab, embed_glove)\n",
    "print(\"Fasttext Coverage: \")\n",
    "oov_fasttext = check_coverage(vocab, embed_fasttext)\n",
    "print(\"Paragram Coverage: \")\n",
    "oov_para = check_coverage(vocab, embed_para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e6ec070def93bfd6f8539274aadbf80119b4fead"
   },
   "source": [
    "### Clean Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "9d99da7a44650d930ba1ce02f30cf3829acf68bd"
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\n",
    "                       \"can't\": \"cannot\", \"'cause\": \"because\", \n",
    "                       \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
    "                       \"didn't\": \"did not\",  \"doesn't\": \"does not\", \n",
    "                       \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                       \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                       \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n",
    "                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \n",
    "                       \"how'll\": \"how will\", \"how's\": \"how is\",  \n",
    "                       \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n",
    "                       \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n",
    "                       \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \n",
    "                       \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \n",
    "                       \"i'll've\": \"i will have\",\"i'm\": \"i am\",\n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
    "                       \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n",
    "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "                       \"let's\": \"let us\", \"ma'am\": \"madam\", \n",
    "                       \"mayn't\": \"may not\", \"might've\": \"might have\",\n",
    "                       \"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                       \"must've\": \"must have\", \"mustn't\": \"must not\", \n",
    "                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n",
    "                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                       \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
    "                       \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
    "                       \"shan't've\": \"shall not have\", \"she'd\": \"she would\",\n",
    "                       \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \n",
    "                       \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
    "                       \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                       \"so's\": \"so as\", \"this's\": \"this is\",\n",
    "                       \"that'd\": \"that would\", \"that'd've\": \"that would have\", \n",
    "                       \"that's\": \"that is\", \"there'd\": \"there would\", \n",
    "                       \"there'd've\": \"there would have\", \"there's\": \"there is\", \n",
    "                       \"here's\": \"here is\",\"they'd\": \"they would\",\n",
    "                       \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \n",
    "                       \"they've\": \"they have\", \"to've\": \"to have\", \n",
    "                       \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \n",
    "                       \"we'll've\": \"we will have\", \"we're\": \"we are\", \n",
    "                       \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "                       \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \n",
    "                       \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                       \"when've\": \"when have\", \"where'd\": \"where did\", \n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \n",
    "                       \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                       \"who's\": \"who is\", \"who've\": \"who have\", \n",
    "                       \"why's\": \"why is\", \"why've\": \"why have\", \n",
    "                       \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                       \"won't've\": \"will not have\", \"would've\": \"would have\", \n",
    "                       \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                       \"y'all'd've\": \"you all would have\",\n",
    "                       \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                       \"you'd\": \"you would\", \"you'd've\": \"you would have\", \n",
    "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n",
    "                       \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "153dfe264dc1c48d44ec67eb45567b4682d0ecc6"
   },
   "outputs": [],
   "source": [
    "def known_contractions(embed):\n",
    "    known = []\n",
    "    for contract in contraction_mapping:\n",
    "        if contract in embed:\n",
    "            known.append(contract)\n",
    "    return known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "821b4077286e0ffb898b286ceb21e94281fc6102"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Known Contractions -\n",
      "   Glove :\n",
      "[\"can't\", \"'cause\", \"didn't\", \"doesn't\", \"don't\", \"I'd\", \"I'll\", \"I'm\", \"I've\", \"it's\", \"ma'am\", \"o'clock\", \"that's\", \"you'll\", \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"- Known Contractions -\")\n",
    "print(\"   Glove :\")\n",
    "print(known_contractions(embed_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "5f1d66d465a5ba9ecc89d73e148fff54dda6567a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Known Contractions -\n",
      "   Fasttext :\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(\"- Known Contractions -\")\n",
    "print(\"   Fasttext :\")\n",
    "print(known_contractions(embed_fasttext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "5085bce9d7e20e523b552bc46a5c781a5ee1bbb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Known Contractions -\n",
      "   Paragram :\n",
      "[\"can't\", \"'cause\", \"didn't\", \"doesn't\", \"don't\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"it's\", \"ma'am\", \"o'clock\", \"that's\", \"you'll\", \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"- Known Contractions -\")\n",
    "print(\"   Paragram :\")\n",
    "print(known_contractions(embed_para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "cf9d64270441cd0a0038be0708a5c00684354769"
   },
   "outputs": [],
   "source": [
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "054603bbefdf2349b967ee7ca5888b2ed7422bdb"
   },
   "outputs": [],
   "source": [
    "#Make lower Case (not sure if this is needed)\n",
    "all_data['lowered_text'] = all_data['question_text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "d01126597580e2b8e2276f144be6bc015babb527"
   },
   "outputs": [],
   "source": [
    "# Map contractions to words\n",
    "all_data['question_text'] = all_data['question_text'].apply(lambda x: clean_contractions(x, contraction_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "4b9674bcc1735855cf89b40a527a65d0e2c67f8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Coverage: \n",
      "Found embeddings for 32.915% of vocab\n",
      "Found embeddings for  88.392% of all text\n",
      "Fasttext Coverage: \n",
      "Found embeddings for 29.905% of vocab\n",
      "Found embeddings for  88.229% of all text\n",
      "Paragram Coverage: \n",
      "Found embeddings for 19.451% of vocab\n",
      "Found embeddings for  72.501% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(all_data['question_text'])\n",
    "\n",
    "print(\"Glove Coverage: \")\n",
    "oov_glove = check_coverage(vocab, embed_glove)\n",
    "print(\"Fasttext Coverage: \")\n",
    "oov_fasttext = check_coverage(vocab, embed_fasttext)\n",
    "print(\"Paragram Coverage: \")\n",
    "oov_para = check_coverage(vocab, embed_para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be68e5a30ce64ffe3bb5fdd46245d0e400e7ec9c"
   },
   "source": [
    "### Remove non-printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "28419ec5d70f049643a4b628b83b1f7018cbf669"
   },
   "outputs": [],
   "source": [
    "def remove_non_printable(sentence):\n",
    "    \n",
    "    # remove non printable characters\n",
    "    output = ''.join([x for x in sentence if x in string.printable])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "3fd6f924e89a0a5b786969f46ba4b7e54b80a3c9"
   },
   "outputs": [],
   "source": [
    "all_data[\"question_text\"] = all_data[\"question_text\"].apply(lambda x: remove_non_printable(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "acf1b41f0c52decc04c75d5cdb2d6fd5a48e6052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Coverage: \n",
      "Found embeddings for 33.589% of vocab\n",
      "Found embeddings for  88.449% of all text\n",
      "Fasttext Coverage: \n",
      "Found embeddings for 30.290% of vocab\n",
      "Found embeddings for  88.269% of all text\n",
      "Paragram Coverage: \n",
      "Found embeddings for 19.850% of vocab\n",
      "Found embeddings for  72.542% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(all_data[\"question_text\"])\n",
    "\n",
    "print(\"Glove Coverage: \")\n",
    "oov_glove = check_coverage(vocab, embed_glove)\n",
    "print(\"Fasttext Coverage: \")\n",
    "oov_fasttext = check_coverage(vocab, embed_fasttext)\n",
    "print(\"Paragram Coverage: \")\n",
    "oov_para = check_coverage(vocab, embed_para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8cc6390cc97ece6c0808a1e86488a7004a58172f"
   },
   "source": [
    "### Clean Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "831fd59e5c37057be69f90e0505de487303a76a2"
   },
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "630f796ae0afdb4e40f63ca56f6f6c3e0d3faa44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Unknown Punctuation:\n",
      "“ ” ’ ∞ θ ÷ α • à − β ∅ ³ π ‘ ₹ ´ ° £ € × ™ √ ² — – \n",
      "\n",
      "Fasttext Unknown Punctuation:\n",
      "_ ` \n",
      "\n",
      "Paragram Unknown Punctuation:\n",
      "“ ” ’ ∞ θ ÷ α • à − β ∅ ³ π ‘ ₹ ´ ° £ € × ™ √ ² — – \n"
     ]
    }
   ],
   "source": [
    "def unknown_punct(embed, punct):\n",
    "    unknown = ''\n",
    "    for p in punct:\n",
    "        if p not in embed:\n",
    "            unknown += p\n",
    "            unknown += ' '\n",
    "    return unknown\n",
    "\n",
    "print(\"Glove Unknown Punctuation:\")\n",
    "print(unknown_punct(embed_glove, punct))\n",
    "print()\n",
    "print(\"Fasttext Unknown Punctuation:\")\n",
    "print(unknown_punct(embed_fasttext, punct))\n",
    "print()\n",
    "print(\"Paragram Unknown Punctuation:\")\n",
    "print(unknown_punct(embed_para, punct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "6e5f78341ff26869dc4c43debc7f2c986e035069"
   },
   "outputs": [],
   "source": [
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\",\n",
    "                 \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \n",
    "                 \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", \n",
    "                 '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', \n",
    "                 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', \n",
    "                 '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "496c3c62e284f5c77550883d170a6ce9ed1e4566"
   },
   "outputs": [],
   "source": [
    "def clean_special_chars(text, punct, mapping):\n",
    "    # replace special characters with mapping\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    # space out punctuation\n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    # Other special characters that I have to deal with in last\n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''} \n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "64d7d8d9ac1ec7d3295beb70f7252a1682d9e33a"
   },
   "outputs": [],
   "source": [
    "all_data['question_text'] = all_data['question_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "d8180af6a4c643f8d09ae8a3827b06f0b350c369"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Coverage: \n",
      "Found embeddings for 75.141% of vocab\n",
      "Found embeddings for  99.595% of all text\n",
      "Fasttext Coverage: \n",
      "Found embeddings for 67.063% of vocab\n",
      "Found embeddings for  99.441% of all text\n",
      "Paragram Coverage: \n",
      "Found embeddings for 41.175% of vocab\n",
      "Found embeddings for  83.334% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(all_data[\"question_text\"])\n",
    "\n",
    "print(\"Glove Coverage: \")\n",
    "oov_glove = check_coverage(vocab, embed_glove)\n",
    "print(\"Fasttext Coverage: \")\n",
    "oov_fasttext = check_coverage(vocab, embed_fasttext)\n",
    "print(\"Paragram Coverage: \")\n",
    "oov_para = check_coverage(vocab, embed_para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df746ae37ed0cd83599b5a3e460f5f72744225dc"
   },
   "source": [
    "### Correct Mispellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "30005fb6f2733aebed439d94f82c78a81965e17a"
   },
   "outputs": [],
   "source": [
    "mispell_dict = {'colour': 'color',\n",
    "                'centre': 'center', \n",
    "                'favourite': 'favorite', \n",
    "                'travelling': 'traveling',\n",
    "                'counselling': 'counseling', \n",
    "                'theatre': 'theater', \n",
    "                'cancelled': 'canceled', \n",
    "                'labour': 'labor', \n",
    "                'organisation': 'organization', \n",
    "                'wwii': 'world war two', \n",
    "                'citicise': 'criticize', \n",
    "                'youtu ': 'youtube ', \n",
    "                'Qoura': 'Quora', \n",
    "                'sallary': 'salary', \n",
    "                'Whta': 'What', \n",
    "                'narcisist': 'narcissist', \n",
    "                'howdo': 'how do', \n",
    "                'whatare': 'what are',\n",
    "                'howcan': 'how can', \n",
    "                'howmuch': 'how much', \n",
    "                'howmany': 'how many', \n",
    "                'whydo': 'why do', \n",
    "                'doI': 'do I', \n",
    "                'theBest': 'the best', \n",
    "                'howdoes': 'how does', \n",
    "                'mastrubation': 'masturbation', \n",
    "                'mastrubate': 'masturbate',\n",
    "                \"mastrubating\": 'masturbating', \n",
    "                'pennis': 'penis', \n",
    "                'Etherium': 'Ethereum', \n",
    "                'narcissit': 'narcissist',\n",
    "                'bigdata': 'big data', \n",
    "                '2k17': '2017', \n",
    "                '2k18': '2018', \n",
    "                'qouta': 'quota', \n",
    "                'exboyfriend': 'ex boyfriend', \n",
    "                'airhostess': 'air hostess', \n",
    "                \"whst\": 'what', \n",
    "                'watsapp': 'social medium', \n",
    "                'demonitisation': 'demonetization', \n",
    "                'demonitization': 'demonetization', \n",
    "                'demonetisation': 'demonetization', \n",
    "                'pokémon': 'pokemon',\n",
    "                'didnt':'did not',\n",
    "                'doesnt':'does not',\n",
    "                'isnt':'is not',\n",
    "                'shouldnt':'should not',\n",
    "                'behaviour': 'behavior',\n",
    "                'instagram': 'social medium',\n",
    "                'whatsapp': 'social medium',\n",
    "                'snapchat': 'social medium',\n",
    "                'litre': 'liter',\n",
    "                'flavour': 'flavor',\n",
    "                'humour': 'humor',\n",
    "                'neighbour': 'neighbor',\n",
    "                'apologise': 'aplogize',\n",
    "                'organise': 'organize',\n",
    "                'recognise': 'recognize',\n",
    "                'analyse': 'analyze',\n",
    "                'travelled': 'traveled',\n",
    "                'travelling': 'traveling',\n",
    "                'traveller': 'traveler',\n",
    "                'fuelled': 'fueled',\n",
    "                'fuelling': 'fueling',\n",
    "                'defence': 'defense',\n",
    "                'licence': 'license',\n",
    "                'offence': 'offense',\n",
    "                'pretence': 'pretense',\n",
    "                'analog': 'analogue',\n",
    "                'catalog': 'catalogue',\n",
    "                'dialog': 'dialogue'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "d696eb667959fe33c9d4d40efd660b314bae94dd"
   },
   "outputs": [],
   "source": [
    "def correct_spelling(x, dic):\n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "9a40cb84256c4b29f4a6786b10fcb5cbc054fa33"
   },
   "outputs": [],
   "source": [
    "all_data['question_text'] = all_data['question_text'].apply(lambda x: correct_spelling(x, mispell_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "e695f67b6203c7aa7a6fb878e6db97760ad0bed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Coverage: \n",
      "Found embeddings for 75.127% of vocab\n",
      "Found embeddings for  99.595% of all text\n",
      "Fasttext Coverage: \n",
      "Found embeddings for 67.050% of vocab\n",
      "Found embeddings for  99.441% of all text\n",
      "Paragram Coverage: \n",
      "Found embeddings for 41.141% of vocab\n",
      "Found embeddings for  83.333% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(all_data[\"question_text\"])\n",
    "\n",
    "print(\"Glove Coverage: \")\n",
    "oov_glove = check_coverage(vocab, embed_glove)\n",
    "print(\"Fasttext Coverage: \")\n",
    "oov_fasttext = check_coverage(vocab, embed_fasttext)\n",
    "print(\"Paragram Coverage: \")\n",
    "oov_para = check_coverage(vocab, embed_para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "223eed74c67038159faa42c960aa0b283830de6c"
   },
   "source": [
    "### Remove stopwords, one character words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "8f117fbf5cfef5020a6d35aae8acf55c36b3c946"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_extra(sentence, wordsToRemove):\n",
    "    \n",
    "    # remove stop words\n",
    "    resultwords  = [word for word in sentence.split() if word.lower() not in wordsToRemove]\n",
    "    result = ' '.join(resultwords)\n",
    "    \n",
    "    #remove one character long words\n",
    "    output =  re.sub(r\"\\b[a-zA-Z]\\b\", \"\", result)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "70cb205583adce799c2e137f89fa660ab6f231b0"
   },
   "outputs": [],
   "source": [
    "to_remove = ['a','to','of','and']\n",
    "all_data[\"question_text\"] = all_data[\"question_text\"].apply(lambda x: remove_extra(x, to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7c3094275876e67d8a60513178d1d40cb3ba7939"
   },
   "outputs": [],
   "source": [
    "vocab = build_vocab(all_data[\"question_text\"])\n",
    "\n",
    "print(\"Glove Coverage: \")\n",
    "oov_glove = check_coverage(vocab, embed_glove)\n",
    "print(\"Fasttext Coverage: \")\n",
    "oov_fasttext = check_coverage(vocab, embed_fasttext)\n",
    "print(\"Paragram Coverage: \")\n",
    "oov_para = check_coverage(vocab, embed_para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ac90736cea25cdbd0729fd65d1d01d6d3323d56"
   },
   "source": [
    "## Featue Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0bea913dc16830c26b8194416c02158d6c226691",
    "colab_type": "text",
    "id": "7vi7_uDvPsap"
   },
   "source": [
    "## Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0d9dba7e6dca7cc80a3d2bfb20bb935eae2c0ac",
    "colab": {},
    "colab_type": "code",
    "id": "v1Tk8_95Psar"
   },
   "outputs": [],
   "source": [
    "# Will run 5-fold cross validation evaluated with the F1 Score\n",
    "# model: model to cross-validate \n",
    "# train_set: training set being used to validate model\n",
    "def f1_cv(model, train_set):\n",
    "    kf = KFold(5, shuffle=True, random_state=0).get_n_splits(train_set)\n",
    "    f1_score = cross_val_score(model, train_set, y_train, scoring=\"f1\", cv = kf)\n",
    "    return f1_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e7f45cd8a7086aeff4d93d61fba4bbdf011ad9c2",
    "colab": {},
    "colab_type": "code",
    "id": "7W5WbfPOPsaw"
   },
   "outputs": [],
   "source": [
    "# Generates a submission file from predictions of the given model on the test set\n",
    "def gen_sub(name, model):\n",
    "    y_pred = model.predict(X_test)\n",
    "    solution = pd.DataFrame({\"qid\":test_qid, \"prediction\":y_pred})\n",
    "    solution.to_csv(name +\".csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b9fa1f8f5f7cf3eac3704c7a53bf18a07a20d71",
    "colab_type": "text",
    "id": "gvvSF8eQPsa2"
   },
   "source": [
    "## Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a33d7a60e33309ef612de4733044293ad8ac6a5b",
    "colab": {},
    "colab_type": "code",
    "id": "5TpF0SaSPsa3"
   },
   "outputs": [],
   "source": [
    "# split the data back into original train and test set\n",
    "X_train = all_data[:train_qid.size]\n",
    "X_test = all_data[train_qid.size:]\n",
    "y_train = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "953aa61b615dcd364170b30bc2294ff533e88814",
    "colab": {},
    "colab_type": "code",
    "id": "qdBo6A2tPsa9"
   },
   "outputs": [],
   "source": [
    "# first model code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0089ee964e9ad6846b1076313f43f0762f0ef5fe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b39b217b8737abed6d18adf9c231483ea0eb48d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "37e4efb7615b83d5f314d10a25ceb5ac830ec70c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a6689cc9648066332572754bb08e08dbab8bcc87"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "TermProject.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
